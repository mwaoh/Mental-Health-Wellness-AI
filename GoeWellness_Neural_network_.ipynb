{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dEaVsqSgNyQ"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4FyfuZX-gTKS"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT8AyHRMNh41"
      },
      "source": [
        "# TensorFlow Recommenders: Quickstart\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/quickstart\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f-reQ11gbLB"
      },
      "source": [
        "In this tutorial, we build a simple matrix factorization model using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) with TFRS. We can use this model to recommend movies for a given user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA00wBE2Ntdm"
      },
      "source": [
        "### Import TFRS\n",
        "\n",
        "First, install and import TFRS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yzAaM85Z12D"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3oYt3R6Nr9l"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCxQ1CZcO2wh"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "\n",
        "\n",
        "# links dataset\n",
        "\n",
        "users = pd.read_csv('/content/survey_users.csv')\n",
        "print(users.shape)\n",
        "print(users.info())\n",
        "users.head()\n",
        "\n",
        "\n",
        "\n",
        "# titles dataset\n",
        "\n",
        "titles = pd.read_csv('/content/survey_titles.csv')\n",
        "print(titles.shape)\n",
        "print(titles.info())\n",
        "titles.head()\n",
        "\n",
        "titles.TitleId.nunique()\n",
        "\n",
        "\n",
        "# ratings dataset\n",
        "\n",
        "ratings_df = pd.read_csv('/content/survey_ratings.csv')\n",
        "print(ratings_df.shape)\n",
        "print(ratings_df.info())\n",
        "ratings_df.head()\n"
      ],
      "metadata": {
        "id": "eScTd6iL7G4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets\n",
        "ratingsPath = '/content/survey_ratings.csv'\n",
        "titlePath = '/content/survey_titles.csv'\n",
        "userPath = '/content/survey_users.csv'"
      ],
      "metadata": {
        "id": "51ndUhgz6t9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging titles and ratings datasets to perform EDA\n",
        "\n",
        "merged_df = pd.merge(ratings_df, titles, on = 'TitleId', how = 'outer')\n",
        "merged_df"
      ],
      "metadata": {
        "id": "234MN3BI67-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.drop(columns = 'Unnamed: 0', inplace = True)\n",
        "\n",
        "merged_df.isnull().sum()"
      ],
      "metadata": {
        "id": "qxNzZVDW7gKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkout number of unique data points\n",
        "print('unique users: ', merged_df['UserId'].nunique())\n",
        "print('unique titles: ', merged_df['TitleId'].nunique())\n",
        "print('unique ratings', merged_df['Rating'].unique())"
      ],
      "metadata": {
        "id": "nJl-RXgTJv2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "40s_l1Bd7gFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert user  to sequential numerical values\n",
        "user_ids = merged_df['UserId'].unique()\n",
        "user_id_map = {user_id: i for i, user_id in enumerate(user_ids)}\n",
        "merged_df['UserSeqId'] = merged_df['UserId'].map(user_id_map)\n"
      ],
      "metadata": {
        "id": "SOPFZyzSLMen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert title IDs  to sequential numerical values\n",
        "title_ids = merged_df['TitleId'].unique()\n",
        "title_id_map = {title_id: i for i, title_id in enumerate(title_ids)}\n",
        "merged_df['TitleSeqId'] = merged_df['TitleId'].map(title_id_map)"
      ],
      "metadata": {
        "id": "scOmVev8Lncy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(merged_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ytL5cEvbL3Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "7p_fma_OMLhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "num_users = len(user_ids)\n",
        "num_titles = len(title_ids)\n",
        "embedding_size = 100 # latent features dimensions\n",
        "\n",
        "# prepare the inputs for the neural network\n",
        "user_input = tf.keras.Input(shape=(1,))\n",
        "title_input = tf.keras.Input(shape=(1,))\n",
        "\n",
        "# define the input embeddings\n",
        "user_embedding = tf.keras.layers.Embedding(num_users, embedding_size)(user_input)\n",
        "title_embedding = tf.keras.layers.Embedding(num_titles, embedding_size)(title_input)\n",
        "\n",
        "# falttent the embeddings to reduce the dimensionality\n",
        "user_flatten = tf.keras.layers.Flatten()(user_embedding)\n",
        "title_flatten = tf.keras.layers.Flatten()(title_embedding)\n",
        "\n",
        "# combine the input into a single input\n",
        "concatenated = tf.keras.layers.Concatenate()([user_flatten, title_flatten])\n",
        "\n",
        "# define dense layer 1 and dropout layer 1\n",
        "dense1 = tf.keras.layers.Dense(64, activation='relu')(concatenated)\n",
        "dropout1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
        "\n",
        "# define dense layer 2 and dropout layer 2\n",
        "dense2 = tf.keras.layers.Dense(32, activation='relu')(dropout1)\n",
        "dropout2 = tf.keras.layers.Dropout(0.2)(dense2)\n",
        "\n",
        "# define dense layer 3 and dropout layer 3\n",
        "dense3 = tf.keras.layers.Dense(16, activation='relu')(dropout2)\n",
        "dropout3 = tf.keras.layers.Dropout(0.2)(dense3)\n",
        "\n",
        "# define dense layer 4 and dropout layer 4\n",
        "dense4 = tf.keras.layers.Dense(8, activation='relu')(dropout3)\n",
        "dropout4 = tf.keras.layers.Dropout(0.2)(dense4)\n",
        "\n",
        "# define dense layer 5 and output layer\n",
        "dense5 = tf.keras.layers.Dense(4, activation='relu')(dropout4)\n",
        "output = tf.keras.layers.Dense(1, activation='relu')(dense5)\n",
        "\n",
        "# instantiate the model\n",
        "model = tf.keras.Model(inputs=[user_input, title_input], outputs=output)\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# train the model\n",
        "model_hist = model.fit([train_data['UserSeqId'], train_data['TitleSeqId']], train_data['Rating'],\n",
        "          batch_size=32, epochs=50, validation_data=([test_data['UserSeqId'], test_data['TitleSeqId']], test_data['Rating']))\n",
        "\n",
        "# evaluate the model\n",
        "mse = model.evaluate([test_data['UserSeqId'], test_data['TitleSeqId']], test_data['Rating'])\n",
        "print('Mean Squared Error:', mse)"
      ],
      "metadata": {
        "id": "MXR087pbMnrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neJAJVwbReNd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "# Specify the folder path\n",
        "model_folder_path = 'Models/NN_Current_Iter/'\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(model_folder_path, exist_ok=True)\n",
        "# Save the model architecture\n",
        "model_architecture_path = 'Models/NN_Current_Iter/neural_net_architecture_curr.pkl'\n",
        "with open(model_architecture_path, 'wb') as f:\n",
        "    pickle.dump(model.to_json(), f)\n",
        "\n",
        "# Save the model weights\n",
        "model_weights_path = 'Models/NN_Current_Iter/neural_net_weights_curr.pkl'\n",
        "model.save_weights(model_weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_architecture_path = 'Models/NN_Current_Iter/neural_net_architecture_curr.pkl'\n",
        "with open(model_architecture_path, 'rb') as f:\n",
        "    loaded_model_architecture = pickle.load(f)\n",
        "\n",
        "loaded_model = tf.keras.models.model_from_json(loaded_model_architecture)\n",
        "\n",
        "model_weights_path = 'Models/NN_Current_Iter/neural_net_weights_curr.pkl'\n",
        "loaded_model.load_weights(model_weights_path)"
      ],
      "metadata": {
        "id": "0dQLW79zNzeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to calculate overall precision, recall and f1scores\n",
        "def precision_recall_f1(actual_ratings, predicted_ratings, threshold=3):\n",
        "\n",
        "    # convert the ratings and predictions to binary form based on the threshold\n",
        "    actual_ratings_binary = np.array(actual_ratings) >= threshold\n",
        "    predicted_ratings_binary = np.array(predicted_ratings) >= threshold\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = precision_score(actual_ratings_binary, predicted_ratings_binary)\n",
        "    recall = recall_score(actual_ratings_binary, predicted_ratings_binary)\n",
        "    f1_score = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "lnyGXiAdN1u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import math\n",
        "\n",
        "# make predictions\n",
        "predictions_nn = loaded_model.predict([test_data['UserSeqId'], test_data['TitleSeqId']]).flatten()\n",
        "\n",
        "# compute metrics\n",
        "mse = mean_squared_error(test_data['Rating'], predictions_nn)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(test_data['Rating'], predictions_nn)\n",
        "precision, recall, f1 = precision_recall_f1(np.array(test_data['Rating']), predictions_nn)\n",
        "\n",
        "# display the metrics\n",
        "print('MSE:', mse)\n",
        "print('RMSE:', rmse)\n",
        "print('MAE:', mae)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "D-aoXKRqN7mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to get top_n recommendations above a threshold ratings\n",
        "def get_top_recommendations(model, user_id, n, thres=0):\n",
        "\n",
        "    final_recommendation = []\n",
        "\n",
        "    # get the user's sequential ID\n",
        "    user_seq_id = user_id_map[user_id]\n",
        "\n",
        "    num_users = len(user_ids)\n",
        "    num_titles = len(title_ids)\n",
        "\n",
        "    # get the inputs for the model\n",
        "    title_seq_ids = np.arange(num_titles)\n",
        "    user_seq_ids = np.repeat(user_seq_id, num_titles)\n",
        "\n",
        "    # get the predictions from the neural network\n",
        "    predictions = model.predict([user_seq_ids, title_seq_ids])\n",
        "\n",
        "    # create a DataFrame with title IDs and predicted ratings\n",
        "    recommendations_df = pd.DataFrame({'TitleSeqId': title_seq_ids, 'PredictedRating': predictions.flatten()})\n",
        "\n",
        "    # remove the recommendations that are already seen by the user\n",
        "    seen_title_ids = merged_df[merged_df['UserSeqId'] == user_seq_id]['TitleSeqId'].values\n",
        "    recommendations_df = recommendations_df[~recommendations_df['TitleSeqId'].isin(seen_title_ids)]\n",
        "\n",
        "    # Sort the recommendations by predicted rating in descending order and select the top N titles\n",
        "    top_recommendations = recommendations_df.sort_values(by='PredictedRating', ascending=False)\n",
        "    top_recommendations = top_recommendations[top_recommendations['PredictedRating'] >=thres].head(n)\n",
        "\n",
        "    # add the recommendations and respective predicted ratings as a tuple to a list\n",
        "    for _, row in top_recommendations.iterrows():\n",
        "        title_seq_id = row['TitleSeqId']\n",
        "        predicted_rating = row['PredictedRating']\n",
        "        title_name = merged_df[merged_df['TitleSeqId'] == title_seq_id]['TitleName'].values[0]\n",
        "        final_recommendation.append((title_name, predicted_rating))\n",
        "\n",
        "    return final_recommendation"
      ],
      "metadata": {
        "id": "Jeyj6KUpOXR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the intended number of recommendations for selected user\n",
        "\n",
        "user_id = 35\n",
        "top_n = 10\n",
        "\n",
        "recommendations = get_top_recommendations(loaded_model, user_id, top_n)\n",
        "for i in recommendations:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "p7BLQszqOnoP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}